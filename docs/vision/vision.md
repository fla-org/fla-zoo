# FLA for Vision ğŸ–¼ï¸

<div align="right">
<em>Last updated: 2025-02-26</em>
</div>

> **Note:** `vision` here refers specifically to image classification. Currently, only classification models are implemented to serve as general vision encoders. See training scripts at [SFT](examples/vision/mim.py) and [MIM](examples/vision/mim.pyy) for more details.

## Overview ğŸ“‹

This module implements image classification models based on FLA to:
- Simplify their application as general vision encoders
- Enable easier adoption and comparisons
- Provide a foundation for vision understanding tasks

The architecture is primarily based on Hugging Face's Vision Transformer (ViT) implementation with several FLA-specific customizations.

### Available Model Types

- `xxxForImageClassification` - For image classification tasks
- `xxxForMaskedImageModeling` - For masked image modeling pre-training
- `xxxVisionModel` - Base vision models for custom downstream tasks

## Implementation ğŸ› ï¸

### 1. Code Structure

- Maintains consistency with existing language models in [FLA](https://github.com/fla-org/flash-linear-attention)
- Follows similar patterns and conventions for easier understanding

### 2. Scanning Options

| Scan Type | Operation Flow |
|-----------|---------------|
| **Uni-scan** | `[B, L, D] â†’ FLA â†’ [B, L, D]` |
| **Random-scan** | `[B, L, D] â†’ random shuffle â†’ [B, L, D] â†’ FLA â†’ [B, L, D]` |
| **Flip-scan** | `[B, L, D] â†’ flip â†’ [B, L, D] â†’ FLA â†’ [B, L, D]` |
| **Bi-scan** | `[B, L, D] â†’ flip â†’ [2*B, L, D] â†’ FLA â†’ [2*B, L, D] â†’ combine â†’ [B, L, D]` |
| **Cross-scan** | `[B, L, D] â†’ cross-scan â†’ [4*B, L, D] â†’ FLA â†’ [4*B, L, D] â†’ cross-merge â†’ [B, L, D]` |

> âš ï¸ **Warning:** The latter two options (Bi-scan and Cross-scan) are design choices adopted by some SSM-based vision models. **Enabling them does not guarantee better performance and will reduce hardware efficiency.**

### 3. Technical Details

- Uses mean pooling exclusively for sequence aggregation
- Adapts common components from Hugging Face's ViT implementation:
  - `Embedding` - For patch and position embeddings
  - `Pooler` - For sequence pooling
  - Initialization code for pretrained models

> ğŸ”œ **Coming Soon:** Mamba, Mamba2, and Samba models will be implemented in future versions due to their structural differences.

## Model Compatibility Tests ğŸ§ª

### Test Configuration

| Parameter | Value |
|-----------|-------|
| **Total layers** | 6 |
| **Hybrid setting** | Attention layers at indices 1,3,5 |
| **Default attention mode** | chunk (except for rwkv6) |

### Test Results

| Model | Pure FLA | Hybrid |
|-------|----------|--------|
| **abc** | âŒ CompilationError | âŒ CompilationError |
| **bitnet** | âŒ AttributeError | âŒ AttributeError |
| **deltanet** | âœ… | âœ… |
| **gated_deltanet** | RTX 4060: âŒ<br>A100: âœ… | RTX 4060: âŒ<br>A100: âœ… |
| **gla** | âŒ CompilationError | âŒ CompilationError |
| **gsa** | âœ… | âœ… |
| **hgrn** | âœ… | âœ… |
| **hgrn2** | âœ… | âœ… |
| **linear_attn** | âŒ Matmul Shape error | âŒ Matmul Shape error |
| **retnet** | âœ… | âœ… |
| **rwkv6** | chunk: âŒ<br>fused_recurrent: âœ… | chunk: âŒ<br>fused_recurrent: âœ… |
| **transformer** | âœ… | âœ… |
| **lightnet** | âœ… | âœ… |

> **Note:** Errors primarily stem from respective attention implementations from FLA.